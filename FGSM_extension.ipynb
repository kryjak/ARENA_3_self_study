{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDLmJg5UGAKS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_cM71dzGXUd"
      },
      "outputs": [],
      "source": [
        "# pdb.set_trace()\n",
        "epsilons = [0, .05, .1, .15, .20, .25]\n",
        "# epsilons_prime = [0.2, 0.4, 0.6, 0.8]\n",
        "epsilons_prime = np.linspace(0.25, 0.7, 10)\n",
        "# epsilons_prime = [0.2,0.4]\n",
        "\n",
        "pretrained_model = \"/content/lenet_mnist_model.pth\"\n",
        "\n",
        "use_cuda=True\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "M06UVmYtRCTp"
      },
      "outputs": [],
      "source": [
        "#@title Define the model architecture and load pretrained weights\n",
        "# LeNet Model definition\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "# MNIST Test dataset and dataloader declaration\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,)),\n",
        "            ])),\n",
        "        batch_size=1, shuffle=True)\n",
        "\n",
        "# Define what device we are using\n",
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the network\n",
        "model = Net().to(device)\n",
        "\n",
        "# Load the pretrained model\n",
        "model.load_state_dict(torch.load(pretrained_model, map_location=device))\n",
        "#\n",
        "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Gc_GJWE7Q8Xi"
      },
      "outputs": [],
      "source": [
        "#@title Define FGSM perturbation and denormalise\n",
        "# FGSM attack code\n",
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "    # Collect the element-wise sign of the data gradient\n",
        "    sign_data_grad = data_grad.sign()\n",
        "    # Create the perturbed image by adjusting each pixel of the input image\n",
        "    perturbed_image = image + epsilon*sign_data_grad\n",
        "    # Adding clipping to maintain [0,1] range\n",
        "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
        "    # Return the perturbed image\n",
        "    return perturbed_image\n",
        "\n",
        "# FGSM extension - target a specific class\n",
        "# the only difference is that we SUBTRACT the gradient, because in this case\n",
        "# we are again trying to minimise cross-entropy loss\n",
        "def fgsm_attack_extension(image, epsilon, data_grad):\n",
        "    # Collect the element-wise sign of the data gradient\n",
        "    sign_data_grad = data_grad.sign()\n",
        "    # Create the perturbed image by adjusting each pixel of the input image\n",
        "    perturbed_image = image - epsilon*sign_data_grad\n",
        "    # Adding clipping to maintain [0,1] range\n",
        "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
        "    # Return the perturbed image\n",
        "    return perturbed_image\n",
        "\n",
        "# restores the tensors to their original scale\n",
        "def denorm(batch, mean=[0.1307], std=[0.3081]):\n",
        "    \"\"\"\n",
        "    Convert a batch of tensors to their original scale.\n",
        "\n",
        "    Args:\n",
        "        batch (torch.Tensor): Batch of normalized tensors.\n",
        "        mean (torch.Tensor or list): Mean used for normalization.\n",
        "        std (torch.Tensor or list): Standard deviation used for normalization.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: batch of tensors without normalization applied to them.\n",
        "    \"\"\"\n",
        "    if isinstance(mean, list):\n",
        "        mean = torch.tensor(mean).to(device)\n",
        "    if isinstance(std, list):\n",
        "        std = torch.tensor(std).to(device)\n",
        "\n",
        "    return batch * std.view(1, -1, 1, 1) + mean.view(1, -1, 1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u_g19WatrTb"
      },
      "source": [
        "#Three functions to test FGSM:\n",
        "1. Maximise the loss function to create generic missclassification\n",
        "2. Minimise the loss with fake constant targets (targetting a specific class) - this pushes the logits towards a particular class\n",
        "3. Once we have identified the best $\\varepsilon$ for target missclassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U2biG3iGgmJ"
      },
      "outputs": [],
      "source": [
        "def test( model, device, test_loader, epsilon ):\n",
        "\n",
        "    # Accuracy counter\n",
        "    correct = 0\n",
        "    adv_examples = []\n",
        "\n",
        "    # Loop over all examples in test set\n",
        "    for data, target in test_loader:\n",
        "\n",
        "        # Send the data and label to the device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Set requires_grad attribute of tensor. Important for Attack\n",
        "        data.requires_grad = True\n",
        "\n",
        "        # Forward pass the data through the model\n",
        "        output = model(data)\n",
        "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "\n",
        "        # If the initial prediction is wrong, don't bother attacking, just move on\n",
        "        if init_pred.item() != target.item():\n",
        "            continue\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = F.nll_loss(output, target)\n",
        "\n",
        "        # Zero all existing gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Calculate gradients of model in backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Collect ``datagrad``\n",
        "        data_grad = data.grad.data\n",
        "\n",
        "        # Restore the data to its original scale\n",
        "        data_denorm = denorm(data)\n",
        "\n",
        "        # Call FGSM Attack\n",
        "        perturbed_data = fgsm_attack(data_denorm, epsilon, data_grad)\n",
        "\n",
        "        # Reapply normalization\n",
        "        perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n",
        "\n",
        "        # Re-classify the perturbed image\n",
        "        output = model(perturbed_data_normalized)\n",
        "\n",
        "        # Check for success\n",
        "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if final_pred.item() == target.item():\n",
        "            correct += 1\n",
        "            # Special case for saving 0 epsilon examples\n",
        "            if epsilon == 0 and len(adv_examples) < 5:\n",
        "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
        "        else:\n",
        "            # Save some adv examples for visualization later\n",
        "            if len(adv_examples) < 5:\n",
        "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
        "\n",
        "    # Calculate final accuracy for this epsilon\n",
        "    final_acc = correct/float(len(test_loader))\n",
        "    print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {len(test_loader)} = {final_acc}\")\n",
        "\n",
        "    # Return the accuracy and an adversarial example\n",
        "    return final_acc, adv_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-OXSw6eeU1q"
      },
      "outputs": [],
      "source": [
        "def test_target_missclassification( model, device, test_loader, epsilon, target_class: int = None ):\n",
        "    correct = 0\n",
        "    adv_examples = []\n",
        "    target = torch.tensor([target_class])\n",
        "\n",
        "    # Loop over all examples in test set\n",
        "    for data, label in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        data.requires_grad = True\n",
        "\n",
        "        output = model(data)\n",
        "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if init_pred.item() == target.item():\n",
        "            continue\n",
        "\n",
        "        loss = F.nll_loss(output, target)\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        data_grad = data.grad.data\n",
        "\n",
        "        data_denorm = denorm(data)\n",
        "        perturbed_data = fgsm_attack_extension(data_denorm, epsilon, data_grad)\n",
        "        perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n",
        "\n",
        "        output = model(perturbed_data_normalized)\n",
        "\n",
        "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if final_pred.item() == target.item():\n",
        "            correct += 1\n",
        "            if len(adv_examples) < 5:\n",
        "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
        "\n",
        "    final_acc = correct/float(len(test_loader))\n",
        "    print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct} / {len(test_loader)} = {final_acc}\")\n",
        "\n",
        "    return final_acc, adv_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVelSD-GqKeW"
      },
      "outputs": [],
      "source": [
        "def test_target_missclassification_iterative( model, device, test_loader, epsilon, target_class: int = None, epochs: int = 5 ):\n",
        "    target = torch.tensor([target_class])\n",
        "\n",
        "    correct_across_epochs = []\n",
        "    adv_examples_across_epochs = dict(zip(list(range(epochs)),  [[] for ii in range(epochs)]))\n",
        "\n",
        "    # Loop over all examples in test set\n",
        "    for data, _ in test_loader:\n",
        "        correct = []\n",
        "\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        data.requires_grad = True\n",
        "        output = model(data)\n",
        "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if init_pred.item() == target.item():\n",
        "            continue\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            loss = F.nll_loss(output, target)\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            data_grad = data.grad.data\n",
        "\n",
        "            data_denorm = denorm(data)\n",
        "            perturbed_data = fgsm_attack_extension(data_denorm, epsilon, data_grad)\n",
        "            perturbed_data_normalized = transforms.Normalize((0.1307,), (0.3081,))(perturbed_data)\n",
        "\n",
        "            output = model(perturbed_data_normalized)\n",
        "\n",
        "            final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            if final_pred.item() == target.item():\n",
        "                correct.append(1)\n",
        "                if len(adv_examples_across_epochs[epoch]) < 5:\n",
        "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
        "                    adv_examples_across_epochs[epoch].append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
        "            else:\n",
        "                correct.append(0)\n",
        "\n",
        "        correct_across_epochs.append(correct)\n",
        "\n",
        "    final_acc = np.mean(correct_across_epochs, axis=0)\n",
        "\n",
        "    return final_acc, adv_examples_across_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAaCzV1lGjMl"
      },
      "outputs": [],
      "source": [
        "accuracies = []\n",
        "examples = []\n",
        "\n",
        "# Run test for each epsilon\n",
        "for eps in epsilons:\n",
        "    acc, ex = test(model, device, test_loader, eps)\n",
        "    accuracies.append(acc)\n",
        "    examples.append(ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbsqoHIHhKXu"
      },
      "outputs": [],
      "source": [
        "accuracies_target_misclassification = []\n",
        "examples_target_misclassification = []\n",
        "\n",
        "# Run test for each epsilon\n",
        "for eps in epsilons_prime:\n",
        "    print(f'Current eps: {eps}')\n",
        "    acc, ex = test_target_missclassification(model, device, test_loader, eps, 2)\n",
        "    accuracies_target_misclassification.append(acc)\n",
        "    examples_target_misclassification.append(ex)\n",
        "\n",
        "best_eps_prime = epsilons_prime[np.argmax(accuracies_target_misclassification)]\n",
        "print(f'Best eps is {best_eps_prime}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohAJDYKQqrSq"
      },
      "outputs": [],
      "source": [
        "# best_eps_prime = 0.4\n",
        "acc_iterative, ex_iterative = test_target_missclassification_iterative(model, device, test_loader, best_eps_prime, target_class=2, epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVlIEUHkGksH"
      },
      "outputs": [],
      "source": [
        "def plot_accuracies(epsilons: list[float], accuracies: list[float], title: str = 'Misclassification'):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.plot(epsilons, accuracies, \"*-\")\n",
        "    plt.yticks(np.arange(0, 1.1, step=0.1))\n",
        "    plt.xticks(np.arange(0, 0.8, step=0.1))\n",
        "    plt.title(\"Accuracy vs Epsilon\")\n",
        "    plt.xlabel(\"Epsilon\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def plot_accuracies_iterative(accuracies: list[float], title: str = 'Target misclassification: Class 2'):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.plot(accuracies, \"*-\")\n",
        "    plt.yticks(np.arange(0, 1.1, step=0.1))\n",
        "    plt.xticks(np.arange(0, len(accuracies), step=1))\n",
        "    plt.title(\"Accuracy vs #training epochs\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lp5hu-bm1oO"
      },
      "outputs": [],
      "source": [
        "plot_accuracies(epsilons, accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Kp_jbxFm4yj"
      },
      "outputs": [],
      "source": [
        "plot_accuracies(epsilons_prime, accuracies_target_misclassification, 'Target misclassification: Class 2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HVc23WIO-zB"
      },
      "outputs": [],
      "source": [
        "plot_accuracies_iterative(acc_iterative, 'Target misclassification: Class 2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNfIC0pEYHjB"
      },
      "outputs": [],
      "source": [
        "# Plot several examples of adversarial samples at each epsilon\n",
        "def plot_adversarial_examples(epsilons, examples):\n",
        "    cnt = 0\n",
        "    plt.figure(figsize=(8,10))\n",
        "    for i in range(len(epsilons)):\n",
        "        for j in range(len(examples[i])):\n",
        "            cnt += 1\n",
        "            plt.subplot(len(epsilons),len(examples[0]),cnt)\n",
        "            plt.xticks([], [])\n",
        "            plt.yticks([], [])\n",
        "            if j == 0:\n",
        "                plt.ylabel(f\"Eps: {epsilons[i]}\", fontsize=14)\n",
        "            orig,adv,ex = examples[i][j]\n",
        "            plt.title(f\"{orig} -> {adv}\")\n",
        "            plt.imshow(ex, cmap=\"gray\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_adversarial_examples_iterative(examples):\n",
        "    cnt = 0\n",
        "    plt.figure(figsize=(8,10))\n",
        "    for i in range(len(examples)):\n",
        "        for j in range(len(examples[i])):\n",
        "            cnt += 1\n",
        "            plt.subplot(len(examples),len(examples[0]),cnt)\n",
        "            plt.xticks([], [])\n",
        "            plt.yticks([], [])\n",
        "            if j == 0:\n",
        "                plt.ylabel(f\"Epoch: {list(range(len(examples)))[i]}\", fontsize=14)\n",
        "            orig,adv,ex = examples[i][j]\n",
        "            plt.title(f\"{orig} -> {adv}\")\n",
        "            plt.imshow(ex, cmap=\"gray\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58Mr8PhNmkl9"
      },
      "outputs": [],
      "source": [
        "plot_adversarial_examples(epsilons, examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hft8edN6mmrn"
      },
      "outputs": [],
      "source": [
        "plot_adversarial_examples(epsilons_prime, examples_target_misclassification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrANnFRprOm_"
      },
      "outputs": [],
      "source": [
        "plot_adversarial_examples_iterative(list(ex_iterative.values()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iJBlWOWjtZyr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}